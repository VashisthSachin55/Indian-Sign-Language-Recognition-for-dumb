import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import pyttsx3
import math

# Initialize the text-to-speech engine
engine = pyttsx3.init()
engine.setProperty('rate', 150)    
engine.setProperty('volume', 1.0)

# Initialize the hand detector and video capture
detector = HandDetector(detectionCon=0.8, maxHands=2)
cap = cv2.VideoCapture(1)  # Change index if another camera is used

# Load the gesture classification model
classifier = Classifier("newmodel1/keras_model.h5", "newmodel1/labels.txt")

margin = 20
img_size = 300


labels = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "0", "P", "Q", "R", "S", "T", "U", "V", "X", "Y", "Z"]

# Main loop for processing video frames
while True:
    success, img = cap.read()
    if not success:
        print("Failed to read from camera")
        break

    # Detect hands in the image
    hands, img = detector.findHands(img)

    if hands:
        hand = hands[0]
        x, y, w, h = hand['bbox']  # Get the bounding box of the detected hand

        # Create a white background to store the resized image
        img_white = np.ones((img_size, img_size, 3), np.uint8) * 255

        # Crop the hand region with a margin
        img_crop = img[y - margin:y + h + margin, x - margin:x + w + margin]

        # Check if the crop size is valid
        if img_crop.size > 0:
            aspect_ratio = h / w

            if aspect_ratio > 1:  # Height is greater than width
                k = img_size / h
                w_cal = math.ceil(k * w)
                img_resize = cv2.resize(img_crop, (w_cal, img_size))
                w_gap = math.ceil((img_size - w_cal) / 2)
                img_white[:, w_gap:w_cal + w_gap] = img_resize
            else:  # Width is greater than height
                k = img_size / w
                h_cal = math.ceil(k * h)
                img_resize = cv2.resize(img_crop, (img_size, h_cal))
                h_gap = math.ceil((img_size - h_cal) / 2)
                img_white[h_gap:h_cal + h_gap, :] = img_resize

            # Get the gesture prediction from the classifier
            prediction, index = classifier.getPrediction(img_white, draw=False)

            # Speak the predicted label
            engine.say(labels[index])
            engine.runAndWait()

    # Check if 'q' is pressed to exit the loop
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the camera and close windows
cap.release()
cv2.destroyAllWindows()
